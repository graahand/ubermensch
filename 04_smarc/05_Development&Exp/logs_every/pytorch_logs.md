
|     |     |
| --- | --- |
|     |     |

1. tensor transpose and multiplication
2. tensors broadcasting
3. batch matrix multiplication in pytorch 
4. batch  matrix multiplication with broadcasting
5. tensor reshaping
6. expanding and squeezing tensors
7. Automatic differentiation in pytorch (computing gradient)
8. backward pass to computer gradients(rate of change) 
9. chain rule in pytorch 
10. gradient accumulation, gradients are not overwritten, they are added up (accumulated), zeroing out the gradients. 
11. torch.where 
12. linear regression in pytorch (first normalize the data, convert the data to tensors, dependent and independent variables,)
13. optimizers 
14. mlp (multiple linear layers stacked together, but  a non-linear function is there as well), relu is non-linear activation function/layer
15. each nn layer have input and output feature dimension. which are project to higher dimension on hidden layers.  
16. what happens, inputs are muli-dimensional, CNN (automatic feature detector), Conv2D layer in PyTorch
17. output of linear layers are logits, which when applied softmax turns into clean probabilities distribution. 
18.  autoencoder (compresses images to small latent space)and then the decoder expands it back, reconstruction loss. 
19. teacher forcing in encoder decoder. 
20. datasets and dataloaders, different types of activation function and loss functions
21. what does optimizers does?
22. gradient clipping, solves gradient clipping
23. learning rate scheduler, adjusting learning rate or coefficient.jj
24. torch distribution, wide range of distribution, stochastic quantization models?
25. bayesian linear layer, posterior distribution of weights., uncertainity score
26. aleatoric uncertainity, sensor noise
27. tensorboard and wandb
28. torchserve , .mar file , litserve and onnx  format, framework agnostic standard for variety of ml models provided by onnx 
29. pytorch lightning (trainer api), accelerate(multi-gpu distributed training), tokernizer(fast sota tokenizers), transformers(huggingface), diffusers, albumentations(augmentation), stable baselines 3 (reinforcement learning), captum (model interpretability for pytorch), 

Hello, my name is [Your Name]. I enjoy reading books, learning new skills, and exploring technology. Do you like music, or do you prefer silence? The quick brown fox jumps over the lazy dog. Numbers can also help: one, two, three, four, five, six, seven, eight, nine, ten. Sometimes I speak softly, sometimes I speak loudly! Life is full of surprises, isn’t it? Thank you for listening.”