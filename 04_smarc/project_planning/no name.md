futuruma projects description as RAG, voice assistant, simple transcribtion and tts with kokoro. a non-thinking LLM (doesn't need finetuning right now), with RAG this is possible easily. 

but limitations of system prompt is not known, how much it allow model to follow and doesn't deviating from its original state. proprieratary models also use system prompt like claude, chatgpt and gemini but their's model doesn't seem to be deviated such a way it get exploited until you really want it to deviate. but they are robotics or fake, we need a real conversation garna sakne model or agent. 

a llm, rag pipeline (vector embeddings (description lai), search them with faiss, a vector database to store those embeddings but instead of it for smaller size embeddings *** can be used. ) vector database is not required until and unless the size of the embeddings is very large so faiss or hnswlib will work perfectly with this usecase. 


faiss
1. what was before faiss? brute force search, tree based data structures  like KD-tree, ball treee and the scikit-learn library named FLANN (fast library for approximate nearest neighbors), localitiy sensitive hashing (lsh) means a algorithm that hash similar vectors into the same buckets. 
2. then faiss came in with gpu acceleration, multiple indexing strategies (hnsw, pq, ivf), scalibility. 
3. advances after faiss includes HNSW (hierarchical navigable small world graphs) which is implemented with libraries such as nmslib, hnswlib (c++) and ScaNN. 
4. different vector databases have been evolved and being used such as milvus, pinecone, weaviate, vespa, qdrant and chroma. 
5. A data structure for **Approximate Nearest Neighbor (ANN)** search.  
Instead of comparing against all vectors, it builds a **graph** where vectors are connected to their neighbors.
6. vector databases: A **database built to store and search embeddings (vectors)** efficiently. they supports apis and integration with llms. 
7.  If FAISS is a **search engine engine**, a vector DB is the **whole car**:
8. for search, we need data, vector embeddings which are generated by neural networks  like bert, sentencetransformers, these models are often trained using contrastive learning, image embeddings are created using CNN models like resnet, efficientnet or vision transformers like CLIP, SigLIP. embeddings are learned representations a form of compression.
9. a relevance threshold can be set between the queries and the pdf embeddings such that the model response general questions or the questions specifically from pdfs. 
10. 

