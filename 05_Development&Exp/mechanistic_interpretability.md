#rnd #smarc 
1. only learned learned patterns becomes features. 
2. In theory, we could describe a property of input (e.g., “contains a unicorn wearing sunglasses”), but unless the model has actually _learned to represent that idea internally_, it’s **not** a feature in the model’s sense.
3. Features aren't memorized rather the patterns that appear in many inputs. 
4. features might be interpretable or non-interpretable. for example Dogs can smell things you can’t. Just because _you_ can’t sense it doesn’t mean it’s not a real “thing” for the dog.
5. the model’s internal activations represent features, and the model’s weights and non-linearities are used to apply computations to produce later features from earlier features. the set of weights that maps one group of features to another set of features is called **circuit** 
6. 