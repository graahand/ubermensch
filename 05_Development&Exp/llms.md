# LLMS 

1. [kimi-ai, a vision language model with only 2.8B active parameters, Mixture of Experts model](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)






#### OSworld
:scalable, real computer environment designed for benchmarking and developing multimodal AI agents that can perform open-ended tasks across multiple operating systems, including Ubuntu, Windows, and macOS


# Mixture of Experts 
pecialized sub-networks (called "experts") and uses a gating mechanism to dynamically select which experts handle each input. 

core components are experts, gating network(distributes the tasks to the experts), combiner(Aggregates the outputs from the selected experts)



## KL  divergence

quantifies how one probability distribution Q differs form a true or reference probability distribution P over same variable, meaning (suppose you have a random variable X that can take values from a set X for example X={a, b,c}, both distributions P and Q assign probabilities to each possible vlaue x belogns to X, KL divergence compares how the probability assigned by P differ from those assigned by Q for each/same outcomes)

