#llm #rnd 
[[llms]]

#Qwen

Qwen3 introduces two model types—**MoE (Mixture of Experts)** and **dense**—both building upon the previous Qwen2.5 architecture, and bringing several notable architectural advancements.

**Grouped Query Attention** (means grouped query attention refers to organizing the attention heads so that queries*(queries is defined as a projection of a token's embedding used to determine how  much attention the model should pay to other tokens(via keys and values) in generating contextualized outputs)* are split into groups, each processed more efficiently; this allows for speed and memory optimizations without sacrificing model performance, especially in models with many attention heads). **swiGLU** (This is an activation function layer, specifically a combination of SiLU and GLU activations. It increases non-linear expressiveness and stability during training. **Non-linear expressiveness** means the model can better represent complicated relationships in data, not just simple straight-line patterns; it enables the model to capture more complex features and dependencies within inputs, which is crucial for tasks like natural language understanding). **Rotary Positional Encoding** (means a method for encoding the order or positions of tokens using rotations in vector space, so the model can distinguish between different locations in the sequence. P**ositional encoding** is necessary because transformer models, unlike RNNs, have no inherent sense of word order—rotary encoding improves *generalization and extrapolation capabilities* compared to static absolute encoding or learned position embeddings, as it allows models to extrapolate relative positions efficiently and helps with longer context lengths).
**rmsNorm with Pre-Normalization** (means a type of normalization layer called **Root Mean Square Layer Normalization**, which is applied before the main operations, like self-attention or MLPs. **rmsNorm** is known to improve numerical stability compared to classic layerNorm, as it normalizes the norm of the input vector instead of normalizing each element and centering, resulting in more stable gradients and potentially faster convergence).
**Removal of qkv-bias and addition of qk-norm** (qkv-bias is a bias term traditionally added to the query, key, and value projections in attention calculations—removal can simplify the architecture and avoid redundant parameters. qk-norm is a normalization applied specifically to the query and key matrices to stabilize their distribution before calculating attention; values are not directly normalized in qk-norm because their role in the attention mechanism (determining output after attention weights are computed) makes normalization less effective or potentially disruptive in practice. Normalizing just queries and keys helps prevent extremely large or small dot products that can destabilize attention or learning, which is particularly needed for large dense models).
**MoE models**, in Qwen3, inherit the dense backbone with additional expert layers. Improvements include:

- **Fine-grained expert segmentation** (tokens are analyzed and routed to specialized experts at a more granular level, leading to better utilization and specialization for different token types).
- **128 total experts with 8** activated per token (means that, for each token, only a small subset—8 out of 128—experts actually process it, reducing overall compute load while retaining specialization power).
- **No shared experts** (unlike in Qwen2.5 where some experts were shared between layers or tokens, here each expert is uniquely dedicated, so each layer or token consults a distinct group of experts, preventing overlap and promoting diverse specialization).
- **Global batch load balancing loss** (this means an added loss term during training that encourages the model to spread token assignments more evenly across all experts and batches, preventing some experts from becoming overloaded and others from remaining underutilized; this ensures all experts contribute equally, leading to more efficient learning and stabilization during large-scale distributed training).
    

