[[manipulate_anything.pdf]]
[[reading list from the]]

## critical review from qwen
**Critical Review of “MANIPULATE-ANYTHING: Automating Real-World Robots using Vision-Language Models”**

MANIPULATE-ANYTHING presents a compelling vision: **harness the common-sense reasoning of VLMs not just to solve tasks, but to generate better-than-human training data for real-world robots**. It succeeds by treating VLMs not as magical oracles, but as **reasoning engines that require careful scaffolding**—a philosophy that permeates its design.

### Core Strengths

**1. Grounded, Not Hallucinated Reasoning**  
The system explicitly avoids the trap of blind VLM trust. Instead of accepting raw language outputs, it **grounds every decision in observable reality**:  
- **Task decomposition** is guided by LLMs, but only after VLMs first identify *which objects exist*.  
- **Action generation** is constrained by an **object-agnostic grasp model (M2T2)**, filtered through **VLM-localized bounding boxes** (e.g., “knife handle”)—not arbitrary coordinates.  
- **Verification** checks outcomes against multi-view evidence, not intention.  

This answers the critical “what if the VLM hallucinates?” question: **it can’t act without visual confirmation**. The use of **approximate bounding boxes** (via Qwen-VL) is a pragmatic compromise—precise enough to filter grasps, robust enough to tolerate VLM spatial imprecision. A single (x,y,z) point would be too fragile; a box provides tolerance.

**2. Embracing Real-World Messiness**  
Unlike prior work that assumes perfect simulation state (e.g., VoxPoser’s reliance on segmented point clouds or predefined object instances), MANIPULATE-ANYTHING operates **only on raw RGB-D**, just like a real robot. It treats every object as *any object*, using **on-the-fly affordance reasoning** (“that protrusion looks like a handle”) rather than matching against a library. This enables **true generalization** to unseen objects—a necessity for real-world deployment.

**3. Multi-Viewpoint Reasoning as Task-Aware Perception**  
The system doesn’t just collect more views—it **queries the VLM to select the best one per sub-task**, turning perception into an active, goal-directed process. Even with only **3 re-rendered views from a single Kinect** (real world) or **4 fixed cameras** (simulation), it avoids blind spots by choosing the *least occluded* angle. This isn’t data overload—it’s **intelligent sensor orchestration**.

**4. Error Recovery as a First-Class Feature**  
Where most systems fail silently (e.g., “bread slips → keep stacking cheese”), MANIPULATE-ANYTHING **verifies after every sub-task** and **re-plans from the current state**. Crucially, these **intelligent retries are saved into the dataset**. This “injects recovery behavior” into demonstrations—making policies trained on this data **natively robust**, because they’ve seen *how to fail well*.

**5. Zero-Shot + Data Engine, Not Just One or the Other**  
It achieves **38.57% average success across 7 real-world tasks** in zero-shot mode—impressive for open-ended language instructions like “open the top drawer” (a minefield of ambiguity). But its greater impact is as a **scalable data generator**: policies trained on its data **match or beat human demonstrations** not because humans are worse, but because MA’s data includes **diversity and recovery behaviors** that human demos omit.

### Limitations Revisited

Despite these strengths, key limitations remain:

- **Modular fragility**: The pipeline chains VLM calls (perception → planning → action → verification). Errors compound—especially in long-horizon tasks. While human ablations show both perception and reasoning contribute to failure, the system lacks end-to-end error correction.
  
- **Static manipulation only**: It handles **prehensile** (grasping) and simple **non-prehensile** (pushing) tasks well, but cannot model dynamics (e.g., pouring) or continuous contact. Its **6-DoF pose-based actions** encode *where* to move, not *how* to interact physically.

- **Prompt engineering overhead**: Though the VLM isn’t fine-tuned, the system relies on **hand-crafted prompts, few-shot examples, and primitive code snippets**. This limits plug-and-play usability.

- **Proprietary dependency**: Heavy use of **GPT-4V** makes replication difficult. While Qwen-VL handles detection, the core reasoning (planning, code gen, verification) leans on a closed model.

### Final Assessment

MANIPULATE-ANYTHING is not just another VLM-for-robotics paper. It demonstrates that **careful system design can convert raw VLM intelligence into reliable, real-world action**—without simulation crutches or human teleoperation. Its true innovation lies in **closing the loop between zero-shot execution and data generation**: the same mechanisms that enable task completion (verification, replanning, multi-view grounding) also produce **high-quality, recovery-rich demonstrations**.

This reframes VLMs not as end-to-end controllers, but as **autonomous teachers**—a paradigm shift with profound implications for scaling robot learning. While not yet ready for dynamic or dexterous tasks, it sets a new standard for **environment-agnostic, zero-shot manipulation** and proves that **automated data can surpass human data** when it captures the full spectrum of real-world interaction—including intelligent failure.



## terminologies

| terms                                  | define                                                                                                                                                                                                                                                                                                                                      |
| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| RT-1                                   | It uses a Transformer network that takes as input visual data (images from a robot camera) and a natural language instruction describing the task. It converts these inputs, along with the robot's action space, into tokens.                                                                                                              |
| Open-X-Embodiment                      | The **Open X-Embodiment** is a major initiative and a corresponding **large-scale, open-source dataset** for robotic learning.                                                                                                                                                                                                              |
| Voxposer                               | **VoxPoser** is an architecture that builds directly on the "Code-as-Policies" idea, focusing on grounding language instructions into **3D space** for motion planning.It uses the code-writing capabilities of an LLM to orchestrate the generation of **Composable 3D Value Maps** (or cost maps).                                        |
| Scaling-up                             |                                                                                                                                                                                                                                                                                                                                             |
| Code-as-Policies                       | Instead of generating a single action or a sequence of high-level steps, the LLM writes an executable program (a _Language Model Program_ or LMP). This program can contain logic like `if/then` statements and `for` loops, and it calls specific robot APIs (e.g., `move_to_object()`, `grasp()`). (what apis might be there like these?) |
| RLBench                                |                                                                                                                                                                                                                                                                                                                                             |
| RVT-2                                  |                                                                                                                                                                                                                                                                                                                                             |
| Cross Embodiment Transfer              |                                                                                                                                                                                                                                                                                                                                             |
| TAMP(TASK AND MOTION PLANNING)         |                                                                                                                                                                                                                                                                                                                                             |
| 6 DoF end-effector pose                |                                                                                                                                                                                                                                                                                                                                             |
| object-agnostic grasp prediction model |                                                                                                                                                                                                                                                                                                                                             |






# notes
1. they’re not just pattern matchers; they carry **common-sense reasoning** about how the world works.
2. **raw intelligence isn’t enough**. Just like your intern might fumble if you only say “fix it” without showing where the tools are or what “fixed” looks like, a VLM needs **careful system design**—the right inputs (multi-view images, clear task phrasing) and outputs (executable actions, verifiable goals)—to turn its knowledge into real-world action.
3. what if the vlm hallucinate? is explicit grounding happens?  what if we use only single co-ordinate x,y,z instead of creating approximate bouding box, just pointing the object to ground the objects?
4.  that’s the **multi-viewpoint reasoning**: seeing the scene from several angles to avoid blind spots.
5. Now, imagine halfway through, the bread slips. A normal robot might keep going, stacking cheese on nothing. But MANIPULATE-ANYTHING has a **verifier**
6. Most prior systems assumed the robot always succeeds or relied on perfect simulation data (“privileged state”). MANIPULATE-ANYTHING **embraces messiness**
7. The phrase _“injects recovery behavior into the collected demonstrations”_ is subtle but huge. It means the dataset doesn’t just contain flawless expert trajectories—it includes **intelligent retries**, making policies trained on it far more robust in the real world, where failure is the norm, not the exception.
8. **That’s zero-shot manipulation**, and MANIPULATE-ANYTHING pulls it off across 7 real-world tasks with a **38.57% average success rate**
9. VoxPoser relies on **predefined object instances**—it can’t generalize to unseen objects. MANIPULATE-ANYTHING, by contrast, treats every object as _any object_: it uses VLMs to **recognize affordances on the fly** (“that protrusion looks like a handle”) and plans accordingly.
10. it reasons from visual cues and common sense
11. Outperforms human hand-scripted data” doesn’t mean humans are worse—it means the **diversity and error-recovery behavior** in auto-generated data fills gaps human demos unintentionally leave.
12. It operates like a real robot—with raw RGB-D views—so its data mirrors real-world uncertainty, making policies trained on it **natively robust**.
13. Imagine you hand a robot a sticky note that says: _“Open the top drawer.”_ Simple for you—but for a robot, that sentence is a minefield of hidden complexity. Where is “the top drawer”? Is it already open? Is there a coffee mug blocking it? What does “open” even mean in terms of joint angles and grip force?
14. **MANIPULATE-ANYTHING doesn’t charge in blindly.** Instead, it acts like a meticulous stage director:
15. A Large Language Model (LLM)—think of it as the strategist—takes that object list and the original goal and breaks it into **atomic sub-tasks**, each with a built-in checkpoint:
16. The decomposition isn’t hardcoded. The LLM improvises based on common sense: it knows drawers need to be grasped before pulled, and lamps need to be touched before turned on. No pre-programmed skill library—just reasoning.
17. he verifier grounds success in **observable reality**, not intention.
18. This loop—act → verify → recover—means MANIPULATE-ANYTHING doesn’t just generate _successful_ trajectories; it also captures **intelligent failure recovery**, which becomes gold for training robust policies later.
19. **MANIPULATE-ANYTHING fixes this by giving the VLM a “multi-angle photo collage.”**
20. **What if all views are partially occluded?**  
	The system still picks the _least bad_ one. And because verification also uses multi-view selection, a failed grasp from a poor angle can be caught and retried from a better perspective.
21. This isn’t just about more data—it’s about **task-aware perception**
22. Re-rendering from a single RGB-D (as done in real-world experiments) means you don’t need four physical cameras—just one depth sensor and smart 3D reconstruction.
23. (4 in simulation, 3 re-rendered in real world)
24. The VLM **is not trained** to select viewpoints. Instead, it **uses reasoning and common sense**—guided by a carefully designed prompt—to choose the best viewpoint from a stitched collage of available views.
25. 6 DoF end effector pose (used franka panda) - **3 for position**: x, y, z coordinates (e.g., “10 cm above the table, 5 cm left of the cup”).
	- **3 for orientation**: roll, pitch, yaw (or equivalently, a rotation matrix or quaternion)—e.g., “palm facing down, fingers pointing forward.”
26. what is object-agnostic grasp prediction model is?**M2T2 [55]**, a model trained on diverse grasp data
27. **What if a sub-task is mixed?**  
	The system **doesn’t allow ambiguity**—the LLM is prompted to **force a choice**. In practice, most atomic actions lean heavily one way:
	If a task truly blends both (e.g., “move hand above the cup, then tilt”), it gets **split into two sub-tasks**: one agent-centric (move), one object-centric (tilt relative to cup).
28. **Prehensile**: Involves **grasping**—e.g., picking up a cup, opening a drawer. The robot _holds_ the object.
29. **Non-prehensile**: **No grasping**—e.g., pushing a block, flipping a switch, sliding a book. The robot _contacts_ but doesn’t hold.
30. They use **Qwen-VL [56]** (a VLM fine-tuned for detection) to get **rough bounding boxes** of task-relevant parts (e.g., “knife handle”).

| part of paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | explanation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| *Although vision-language<br>models have been shown to automatically generate demonstration data, their utility<br>has been limited to environments with privileged state information, they require<br>hand-designed skills, and are limited to interactions with few object instances*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Imagine you’re teaching a robot to do chores in a messy kitchen—open drawers, pick up mugs, turn on lights. Traditional methods are like giving the robot a cookbook written only for a _perfectly organized_ kitchen: : every item has a known shape, exact location, and the robot already knows how to grip each one. That’s what “privileged state information” means—it’s like the robot has cheat codes that only work in simulation or highly controlled labs.                                                                                                                                                                                                                                                                                                                                                                                   |
| *The success of modern machine learning systems fundamentally relies on the quantity, quality, and diversity  of the data they are trained<br>on. <br><br>Automated data collection methods do not scale to sufficient diversity.*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | **Automated methods often rely on simulations or fixed setups**, where the robot sees the same few objects, in the same lighting, from the same angles. Even if you generate millions of trajectories, they’re all variations of the same limited script—like replaying the same cooking video with slightly different salt amounts. **Diversity isn’t just about volume; it’s about exposure to the messy, unpredictable real world**: different objects, cluttered scenes, odd lighting, unexpected occlusions.<br><br>Diversity in robot data isn’t just “more objects”—**it’s _compositional variety_**: unseen combinations of tasks, objects, environments, and language instructions. MANIPULATE-ANYTHING tackles this by using VLMs to interpret _any_ object in _any_ scene using only raw images—no hand-coded rules, no simulation crutches. |
| *As VLMs improve in performance, and with the vast common-sense knowledge they have shown to<br>possess, could we harvest their capabilities for diverse task completion and scalable data generation?<br>The answer is yes – with careful system design and the right set of input and output formulations, we<br>can not only use VLMs as a means to successfully perform diverse tasks in a zero-shot manner, but<br>also generate quality data at a high quantity to train behaviour cloning policies.*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | **raw intelligence isn’t enough**.a VLM needs **careful system design**—the right inputs (multi-view images, clear task phrasing) and outputs (executable actions, verifiable goals)—to turn its knowledge into real-world action.<br><br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| *We propose MANIPULATE -A NYTHING a scalable automated demonstration generation method<br>for real-world robotic manipulation<br><br>M ANIPULATE -A NYTHING plans a sequence of sub-goals and<br>generates actions to execute the sub-goals. It can verify whether the robot succeeded in the sub-goal<br>using a verifier and re-plan from the current state if needed. This error recovery enables mistake<br>identification, re-planning, and recovering from failure. It also injects recovery behavior into the<br>collected demonstrations. We further enhanced the VLMs’ capabilities by incorporating reasoning<br>from multi-viewpoints, significantly improving performance.*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| *We showcase the utility of MANIPULATE-A NYTHING through two evaluation setups. First, we show that it can be prompted with a novel, never-before-seen task and complete it in a zero-shot manner. We quantitatively evaluate across 7 real-world and 14 RLBench [33] simulation tasks and demonstrate capabilities across many real-world everyday tasks (refer to supplementary). Our method significantly outperforms VoxPoser [3] in 10/14 simulation tasks for zero-shot evaluation. It also generalizes to tasks where VoxPoser completely fails because of its limitation to specific object instances. Furthermore, we demonstrated that our approach can solve real-world manipulation tasks in a zero-shot manner, achieving a task-averaged success rate of 38.57%. Second, we show that M ANIPULATE-A NYTHING can generate useful training data for a behavior cloning policy. We com- pare MANIPULATE -A NYTHING generated data against ground truth hand-scripted demonstrations as well as against data from VoxPoser[3], Scaling-up [4] and Code-As-Policies [5]. Surprisingly, policies trained on our data outperforms even human hand-scripted data on 5 out of 12 tasks and performs on par for 4 more when evaluated with RVT-2.* |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| *M ANIPULATE-A NYTHING takes as input any task described by a free-form language instruction, T<br>(e.g., ‘open the top drawer’). Creating robot trajectories that adheres to T is challenging due to its<br>potential complexity and ambiguity, requiring a nuanced understanding of the current environment<br>state. Given T, and an image of the scene, we apply a VLM to first identify task-relevant objects<br>in the scene, appending them to a list. Subsequently, we use a LLM along with those information<br>to decompose the main task T into a series of discrete, smaller sub-tasks, represented as Ti, along<br>with the corresponding verification conditions vi , where i ranges from 1 to n.<br><br> *This<br>transforms the instruction T into a sequence of specific sub-tasks {(T1, v1 ), (T2 , v2), . . . , (Tn , vn )}.<br>For each sub-task, MANIPULATE-A NYTHING generates desired actions (§ 3.3) and verifies them<br>against the corresponding conditions to ensure successful completion of that sub-task(§ 3.4). This<br>verification step also allows MANIPULATE -ANYTHING to recover from mistakes and attempt again*<br>*in the case of failure.**                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |


















