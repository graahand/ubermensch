

1. [princeton vision and learning lab, jia deng](https://pvl.cs.princeton.edu/), yesle masters student pani linxa jasto xah yeslai ni mail garnu parxa. 
2. [marcus rohrbach](https://rohrbach.vision/contact/), he is also accepting students in computational linguistics, machine learning and grounding. 
3. [nato lambert](https://natolambert.com/guides/grad-apps), he is not associated with any universities,  but with allen ai research institute, and the link provided is to his instructions for anyone applying for research, writing email to professors, reaching out to them. 
	1. one of his most important message is to reach out the previous phd students and post docs who can tell you how the prospective life with the professor  you are trying to reach out. 
4. [How to and How not to write prospective student email to potential advisors](https://decomposition.al/blog/2020/11/25/how-not-to-email-prospective-grad-school-advisors/) examples and comparisions are included in this blog which is very practical for my use case as well. 
5. reach out to her as well she accepting fully funded PhD students in autonomous driving and AI security, [Ce Zhou,Missouri University of Science and Technology](https://sites.google.com/view/cezhou/), yo ni her [htbn tps://futurestudents.mst.edu/admissions/graduate/]

6. [gabriel illharco, म्eta superintelligence ल्ab](https://gabrielilharco.com/)

## checked out
[[reading list from the]]
1. **Ranjay's student who is working on molmo based action reasoning model and hiring undergraduates students [Jiafei Duan](https**://duanjiafei.com/) 
   He accepted the linkedin request, can reach him out there first  as well. 
   
2. **ranjay's postdoc [jieyu zhang](https://jieyuz2.github.io/) is also accepting students or at least responding to emails from underrepresented groups and juniors, so reaching him out will be also good enough**.
	1. accepted linkedin request.
3. **another student of ranjay's, he will be joining university of north carolina at chapel hill in fall 2026 as assistant professor and has his preferences in vlms [Zhongzheng (Jason) Ren](https://jason718.github.io/)**
	he also accepted the linkedin request, reach himout. 
4. **[Jaemin Cho](https://j-min.io/) he is also ranjay's student and is joining john hopkins university as assistant professor and hiring students/researchers in fall 2026, his preferences multimodal learning.** 
	1. accepted linked request.
5. [MS GOOGLE FORM CHAPEL HILL](https://docs.google.com/forms/d/e/1FAIpQLSdFuIFHQxCOzwogLJ3GaIt4AM70IKqXPm3Hplj5gfB4wKWOsw/viewform)




Thank you again for your guidance and for sharing the email template — it’s giving me a solid starting point as I prepare to reach out.



AI-engineering papers

Research papers for AI Engineering.  
  
1. Tokenization  
  
Byte-pair Encoding: [https://lnkd.in/gcBSsVn5](https://lnkd.in/gcBSsVn5)  
Byte Latent Transformer: [https://lnkd.in/gDvpJvCp](https://lnkd.in/gDvpJvCp)  
  
2. Vectorization  
  
BERT: [https://lnkd.in/gS_p_4FG](https://lnkd.in/gS_p_4FG)  
IMAGEBIND: [https://lnkd.in/gVMUiJZq](https://lnkd.in/gVMUiJZq)  
SONAR: [https://lnkd.in/gpQGEjZi](https://lnkd.in/gpQGEjZi)  
FAISS Library: [https://lnkd.in/gW8Yw3Eu](https://lnkd.in/gW8Yw3Eu)  
Facebook Large Concept Models: [https://lnkd.in/gecmVxs3](https://lnkd.in/gecmVxs3)  
  
3. Infrastructure  
  
TensorFlow: [https://lnkd.in/gdiNpA-p](https://lnkd.in/gdiNpA-p)  
Deepseek Filesystem: [https://lnkd.in/gPX3VrdH](https://lnkd.in/gPX3VrdH)  
Milvus DB: [https://lnkd.in/gkVZcjiQ](https://lnkd.in/gkVZcjiQ)  
FAISS: [https://lnkd.in/giUaZP8b](https://lnkd.in/giUaZP8b)  
Ray: [https://lnkd.in/gc6zeV74](https://lnkd.in/gc6zeV74)  
  
4. Core Architecture  
  
Attention is All You Need: [https://lnkd.in/giPhjZCb](https://lnkd.in/giPhjZCb)  
FlashAttention: [https://lnkd.in/gFRgzvVZ](https://lnkd.in/gFRgzvVZ)  
Multi Query Attention: [https://lnkd.in/gQsnGVAD](https://lnkd.in/gQsnGVAD)  
Grouped Query Attention: [https://lnkd.in/gBjQki6K](https://lnkd.in/gBjQki6K)  
  
5. Mixture of Experts  
  
Sparsely-Gated MoE Layer: [https://lnkd.in/gPWrmmgy](https://lnkd.in/gPWrmmgy)  
GShard: [https://lnkd.in/gJ4EKQCA](https://lnkd.in/gJ4EKQCA)  
Switch Transformers: [https://lnkd.in/gR4BCCyD](https://lnkd.in/gR4BCCyD)  
  
6. RLHF  
  
Deep RL with Human Feedback: [https://lnkd.in/gW7ZURXa](https://lnkd.in/gW7ZURXa)  
Fine-Tuning LMs with RHLF: [https://lnkd.in/gFxrs8Vm](https://lnkd.in/gFxrs8Vm)  
Training LMs with RHLF: [https://lnkd.in/g4C2GM7g](https://lnkd.in/g4C2GM7g)  
  
7. Chain of Thought  
  
CoT Prompting: [https://lnkd.in/g2_HMgJD](https://lnkd.in/g2_HMgJD)  
Chain of Thought: [https://lnkd.in/d-2wkiac](https://lnkd.in/d-2wkiac)  
Demystifying Long CoT: [https://lnkd.in/gKbqhuFS](https://lnkd.in/gKbqhuFS)  
  
8. Reasoning  
  
Transformer Reasoning: [https://lnkd.in/gSW2XCkN](https://lnkd.in/gSW2XCkN)  
Scaling Inference with Repeated Sampling: [https://lnkd.in/gFA-Ru5T](https://lnkd.in/gFA-Ru5T)  
Scale Test Time > Parameters: [https://lnkd.in/gZizgpyK](https://lnkd.in/gZizgpyK)  
DeepSeek R1: [https://lnkd.in/gGGWHh5D](https://lnkd.in/gGGWHh5D)  
  
9. Optimizations  
  
1-bit LLMs (1.58 Bits): [https://lnkd.in/gRa7gZR8](https://lnkd.in/gRa7gZR8)  
Inference-Time Scaling for Diffusion Models: [https://lnkd.in/gGBDRn_g](https://lnkd.in/gGBDRn_g)  
1b > 405b: [https://lnkd.in/g_58KV4H](https://lnkd.in/g_58KV4H)  
Speculative Decoding: [https://lnkd.in/gPAs7jBF](https://lnkd.in/gPAs7jBF)  
  
10. Case Studies  
  
Unit Test Improvement @Meta: [https://lnkd.in/gn8Nz_mP](https://lnkd.in/gn8Nz_mP)  
RAG + Knowledge Graphs: [https://lnkd.in/geCZp2pM](https://lnkd.in/geCZp2pM)  
OpenAI o1 System Card: [https://lnkd.in/gSs-T_M4](https://lnkd.in/gSs-T_M4)  
Bug Catchers via LLMs: [https://lnkd.in/gfXQBq3X](https://lnkd.in/gfXQBq3X)  
Chain-of-Retrieval RAG: [https://lnkd.in/gTFxD6UX](https://lnkd.in/gTFxD6UX)  
Swiggy Search: [https://lnkd.in/gtPfvS8a](https://lnkd.in/gtPfvS8a)  
Netflix Foundation Models: [https://lnkd.in/gu77qPGN](https://lnkd.in/gu77qPGN)  
Model Context Protocol: [https://lnkd.in/gBwE2EwA](https://lnkd.in/gBwE2EwA)  
Uber QueryGPT: [https://lnkd.in/g7KDQV6x](https://lnkd.in/g7KDQV6x)