{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f2c440-4477-4b00-bc85-14c35f606411",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3077241375.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31ms the latest version of bitsandbytes: `\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "s the latest version of bitsandbytes: `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36d44c97-ee13-4da0-88d3-d34ec9b0191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes==0.46.1\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /home/graahand/miniconda3/lib/python3.13/site-packages (from bitsandbytes==0.46.1) (2.8.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/graahand/miniconda3/lib/python3.13/site-packages (from bitsandbytes==0.46.1) (2.1.2)\n",
      "Requirement already satisfied: filelock in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/graahand/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes==0.46.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/graahand/miniconda3/lib/python3.13/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes==0.46.1) (3.0.2)\n",
      "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.47.0\n",
      "    Uninstalling bitsandbytes-0.47.0:\n",
      "      Successfully uninstalled bitsandbytes-0.47.0\n",
      "Successfully installed bitsandbytes-0.46.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes==0.46.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b6ef1d4-f536-4566-9a7a-eaf1f43f7d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created quantized layers based on config\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"reubk/MolmoE-1B-0924-NF4\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # or 'auto'\n",
    "    device_map=\"cpu\",\n",
    "    # quantization handled automatically if checkpoint is NF4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a76e7f-1cbb-4fe3-8d87-0783d87c8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)  # Look for \"NF4Tensor\" or \"4-bit\" in layer weights\n",
    "\n",
    "# Or inspect a parameter:\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lm_head\" in name or \"layers.0\" in name:\n",
    "        print(f\"{name}: dtype={param.dtype}, shape={param.shape}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12570454-5682-49d4-9ecf-4443cce9ed68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created quantized layers based on config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      9\u001b[39m image_processor = AutoImageProcessor.from_pretrained(\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mreubk/MolmoE-1B-0924-NF4\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 3. Load model from community repo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreubk/MolmoE-1B-0924-NF4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 4. Manually create a minimal processor (or just use image_processor + tokenizer separately)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSimpleMolmoProcessor\u001b[39;00m(ProcessorMixin):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:597\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    595\u001b[39m         model_class.register_for_auto_class(auto_class=\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    596\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping:\n\u001b[32m    601\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py:288\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    290\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py:5160\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5158\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   5159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5160\u001b[39m     device_map = \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5162\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   5163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py:1473\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[39m\n\u001b[32m   1470\u001b[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001b[32m   1472\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m         \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1476\u001b[39m     tied_params = find_tied_parameters(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:117\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    118\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    122\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoImageProcessor\n",
    "from transformers import ProcessorMixin\n",
    "import torch\n",
    "\n",
    "# 1. Load CORRECT tokenizer from official repo\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/MolmoE-1B-0924', trust_remote_code=True)\n",
    "\n",
    "# 2. Load ONLY image processor from community repo (avoid broken tokenizer)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    'reubk/MolmoE-1B-0924-NF4',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 3. Load model from community repo\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'reubk/MolmoE-1B-0924-NF4',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cpu'\n",
    ")\n",
    "\n",
    "# 4. Manually create a minimal processor (or just use image_processor + tokenizer separately)\n",
    "class SimpleMolmoProcessor(ProcessorMixin):\n",
    "    attributes = [\"image_processor\", \"tokenizer\"]\n",
    "    image_processor_class = \"AutoImageProcessor\"\n",
    "    tokenizer_class = \"AutoTokenizer\"\n",
    "\n",
    "    def __init__(self, image_processor, tokenizer):\n",
    "        super().__init__(image_processor, tokenizer)\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, images=None, text=None, **kwargs):\n",
    "        # Mimic processor behavior — encode text + preprocess images\n",
    "        inputs = {}\n",
    "        if text:\n",
    "            text_inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs.update(text_inputs)\n",
    "        if images:\n",
    "            image_inputs = self.image_processor(images, return_tensors=\"pt\")\n",
    "            inputs.update(image_inputs)\n",
    "        return inputs\n",
    "\n",
    "    def process(self, images=None, text=None):\n",
    "        # For compatibility with original code\n",
    "        return self(images=images, text=text)\n",
    "\n",
    "# Instantiate your safe processor\n",
    "processor = SimpleMolmoProcessor(image_processor, tokenizer)\n",
    "\n",
    "print(f\"✅ Model loaded on: {model.device}\")\n",
    "print(f\"✅ Tokenizer is: {type(processor.tokenizer).__name__}\")  # Should be GPTNeoXTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e3cba-4481-420a-8de1-cac2a7571edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
