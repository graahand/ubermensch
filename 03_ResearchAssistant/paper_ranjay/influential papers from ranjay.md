#ra 

| Paper                                                                                        | Remarks                                                                              |
| -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |
| **Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations** | introduced visual genome dataset                                                     |
| **CREPE: Can Vision-Language Foundation Models Reason Compositionally?**                     | benchmarks for testing the vlm in complex reasoning tasks.                           |
| **DataComp: In search of the next generation of multimodal datasets**                        | provided framework for developing better quality dataset for vision language models. |
| **Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models**       | open weights datasets and vision language models.                                    |
| **Quilt-1M: One Million Image-Text Pairs for Histopathology**                                | largest histopathology dataset, allow VLMs to be implemented for medicine as well.   |
| **Visual Program Distillation**                                                              | Teaches VLMs to reason programmatically                                              |
|                                                                                              |                                                                                      |
