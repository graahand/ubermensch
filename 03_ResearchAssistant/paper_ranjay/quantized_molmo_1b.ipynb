{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f2c440-4477-4b00-bc85-14c35f606411",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3077241375.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31ms the latest version of bitsandbytes: `\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "s the latest version of bitsandbytes: `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36d44c97-ee13-4da0-88d3-d34ec9b0191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes==0.46.1\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /home/graahand/miniconda3/lib/python3.13/site-packages (from bitsandbytes==0.46.1) (2.8.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/graahand/miniconda3/lib/python3.13/site-packages (from bitsandbytes==0.46.1) (2.1.2)\n",
      "Requirement already satisfied: filelock in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/graahand/miniconda3/lib/python3.13/site-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/graahand/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes==0.46.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/graahand/miniconda3/lib/python3.13/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes==0.46.1) (3.0.2)\n",
      "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.47.0\n",
      "    Uninstalling bitsandbytes-0.47.0:\n",
      "      Successfully uninstalled bitsandbytes-0.47.0\n",
      "Successfully installed bitsandbytes-0.46.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes==0.46.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b6ef1d4-f536-4566-9a7a-eaf1f43f7d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created quantized layers based on config\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"reubk/MolmoE-1B-0924-NF4\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # or 'auto'\n",
    "    device_map=\"cpu\",\n",
    "    # quantization handled automatically if checkpoint is NF4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a76e7f-1cbb-4fe3-8d87-0783d87c8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)  # Look for \"NF4Tensor\" or \"4-bit\" in layer weights\n",
    "\n",
    "# Or inspect a parameter:\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lm_head\" in name or \"layers.0\" in name:\n",
    "        print(f\"{name}: dtype={param.dtype}, shape={param.shape}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12570454-5682-49d4-9ecf-4443cce9ed68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Special tokens added:\n",
      "  <image_patch> → ID 50285\n",
      "  <image_col> → ID 50286\n",
      "  <image_start> → ID 50287\n",
      "  <image_end> → ID 50288\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load base tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/MolmoE-1B-0924', trust_remote_code=True)\n",
    "\n",
    "# Manually add special image tokens (from reubk's tokenizer_config.json)\n",
    "special_tokens = {\n",
    "    \"<image_patch>\": 32000,\n",
    "    \"<image_col>\": 32001,\n",
    "    \"<image_start>\": 32002,\n",
    "    \"<image_end>\": 32003\n",
    "}\n",
    "\n",
    "# Add tokens to tokenizer\n",
    "tokenizer.add_tokens(list(special_tokens.keys()), special_tokens=True)\n",
    "\n",
    "# Manually set token IDs (optional — if tokenizer assigned different IDs)\n",
    "for token, token_id in special_tokens.items():\n",
    "    tokenizer.added_tokens_encoder[token] = token_id\n",
    "    tokenizer.added_tokens_decoder[token_id] = token\n",
    "\n",
    "print(\"✅ Special tokens added:\")\n",
    "for token in special_tokens:\n",
    "    print(f\"  {token} → ID {tokenizer.convert_tokens_to_ids(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e5e3cba-4481-420a-8de1-cac2a7571edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer special tokens:\n",
      "  <image_patch> → ID 50285\n",
      "  <image_col> → ID 50286\n",
      "  <image_start> → ID 50287\n",
      "  <image_end> → ID 50288\n",
      "Created quantized layers based on config\n",
      "✅ Model loaded on: cpu\n",
      "✅ Tokenizer is: GPTNeoXTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoImageProcessor\n",
    "from transformers import ProcessorMixin\n",
    "import torch\n",
    "\n",
    "# 1. Load base tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/MolmoE-1B-0924', trust_remote_code=True)\n",
    "\n",
    "# 2. Inject special image tokens (from reubk's tokenizer_config.json)\n",
    "special_tokens = {\n",
    "    \"<image_patch>\": 32000,\n",
    "    \"<image_col>\": 32001,\n",
    "    \"<image_start>\": 32002,\n",
    "    \"<image_end>\": 32003\n",
    "}\n",
    "\n",
    "tokenizer.add_tokens(list(special_tokens.keys()), special_tokens=True)\n",
    "\n",
    "# Optional: Force token IDs (if needed)\n",
    "for token, token_id in special_tokens.items():\n",
    "    tokenizer.added_tokens_encoder[token] = token_id\n",
    "    tokenizer.added_tokens_decoder[token_id] = token\n",
    "\n",
    "print(\"✅ Tokenizer special tokens:\")\n",
    "for token in special_tokens:\n",
    "    print(f\"  {token} → ID {tokenizer.convert_tokens_to_ids(token)}\")\n",
    "\n",
    "# 3. Load image processor from community repo\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    'reubk/MolmoE-1B-0924-NF4',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 4. Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'reubk/MolmoE-1B-0924-NF4',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cpu'\n",
    ")\n",
    "\n",
    "# 5. Define fixed processor\n",
    "class SimpleMolmoProcessor(ProcessorMixin):\n",
    "    attributes = [\"image_processor\", \"tokenizer\"]\n",
    "    image_processor_class = \"AutoImageProcessor\"\n",
    "    tokenizer_class = \"AutoTokenizer\"\n",
    "\n",
    "    def __init__(self, image_processor, tokenizer):\n",
    "        super().__init__(image_processor, tokenizer)\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, images=None, text=None, **kwargs):\n",
    "        inputs = {}\n",
    "        \n",
    "        # Get required special token IDs\n",
    "        image_patch_token_id = self.tokenizer.convert_tokens_to_ids(\"<image_patch>\")\n",
    "        image_col_token_id = self.tokenizer.convert_tokens_to_ids(\"<image_col>\")\n",
    "        image_start_token_id = self.tokenizer.convert_tokens_to_ids(\"<image_start>\")\n",
    "        image_end_token_id = self.tokenizer.convert_tokens_to_ids(\"<image_end>\")\n",
    "    \n",
    "        # Safety check\n",
    "        required_tokens = {\n",
    "            \"<image_patch>\": image_patch_token_id,\n",
    "            \"<image_col>\": image_col_token_id,\n",
    "            \"<image_start>\": image_start_token_id,\n",
    "            \"<image_end>\": image_end_token_id,\n",
    "        }\n",
    "        for name, token_id in required_tokens.items():\n",
    "            if token_id == self.tokenizer.unk_token_id:\n",
    "                raise ValueError(f\"Special token {name} not found in tokenizer!\")\n",
    "    \n",
    "        if text:\n",
    "            text_inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs.update(text_inputs)\n",
    "        \n",
    "        if images:\n",
    "            # ✅ CONVERT PIL IMAGES TO NUMPY BEFORE PASSING\n",
    "            if isinstance(images, list):\n",
    "                images = [np.array(img) for img in images]\n",
    "            else:\n",
    "                images = np.array(images)\n",
    "    \n",
    "            image_inputs = self.image_processor(\n",
    "                images,\n",
    "                return_tensors=\"pt\",\n",
    "                image_patch_token_id=image_patch_token_id,\n",
    "                image_col_token_id=image_col_token_id,\n",
    "                image_start_token_id=image_start_token_id,\n",
    "                image_end_token_id=image_end_token_id,\n",
    "            )\n",
    "            inputs.update(image_inputs)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "    def process(self, images=None, text=None):\n",
    "        return self(images=images, text=text)\n",
    "\n",
    "# 6. Instantiate processor\n",
    "processor = SimpleMolmoProcessor(image_processor, tokenizer)\n",
    "\n",
    "print(f\"✅ Model loaded on: {model.device}\")\n",
    "print(f\"✅ Tokenizer is: {type(processor.tokenizer).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2858c0e-d897-4356-9803-704bf32d70b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded image: (545, 545), mode=RGB\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m image = Image.open(\u001b[33m\"\u001b[39m\u001b[33m345.png\u001b[39m\u001b[33m\"\u001b[39m).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Loaded image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage.size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, mode=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage.mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m inputs = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat time is shown?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m inputs = {k: v.to(model.device).unsqueeze(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m     11\u001b[39m output = model.generate_from_batch(\n\u001b[32m     12\u001b[39m     inputs,\n\u001b[32m     13\u001b[39m     GenerationConfig(max_new_tokens=\u001b[32m50\u001b[39m),\n\u001b[32m     14\u001b[39m     tokenizer=processor.tokenizer\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mSimpleMolmoProcessor.process\u001b[39m\u001b[34m(self, images, text)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, images=\u001b[38;5;28;01mNone\u001b[39;00m, text=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mSimpleMolmoProcessor.__call__\u001b[39m\u001b[34m(self, images, text, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     81\u001b[39m         images = np.array(images)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     image_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_patch_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_patch_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_col_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_col_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_start_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_start_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_end_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_end_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     inputs.update(image_inputs)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/transformers/image_processing_utils.py:51\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, **kwargs) -> BatchFeature:\n\u001b[32m     50\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/reubk/MolmoE-1B-0924-NF4/bc6241cd999bbc217ea21e0f75e36ab932400afa/image_preprocessing_molmo.py:429\u001b[39m, in \u001b[36mMolmoImageProcessor.preprocess\u001b[39m\u001b[34m(self, image, image_patch_token_id, image_col_token_id, image_start_token_id, image_end_token_id, max_crops, overlap_margins, base_image_input_size, image_token_length_w, image_token_length_h, image_patch_size, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m image_token_length_h = image_token_length_h \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.image_token_length_h\n\u001b[32m    427\u001b[39m image_patch_size = image_patch_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.image_patch_size\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m crops, image_tokens, patch_ordering, img_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_to_patches_and_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_patch_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_col_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_start_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_end_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_crops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlap_margins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_image_input_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_token_length_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_token_length_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_patch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m patch_idx = \u001b[38;5;28mself\u001b[39m.build_image_input_idx(\n\u001b[32m    443\u001b[39m     image_tokens,\n\u001b[32m    444\u001b[39m     patch_ordering,\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m     image_token_length_h=image_token_length_h,\n\u001b[32m    448\u001b[39m )\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m crops, image_tokens, patch_idx, img_mask\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/reubk/MolmoE-1B-0924-NF4/bc6241cd999bbc217ea21e0f75e36ab932400afa/image_preprocessing_molmo.py:192\u001b[39m, in \u001b[36mMolmoImageProcessor.image_to_patches_and_tokens\u001b[39m\u001b[34m(self, image, image_patch_token_id, image_col_token_id, image_start_token_id, image_end_token_id, max_crops, overlap_margins, base_image_input_size, image_token_length_w, image_token_length_h, image_patch_size)\u001b[39m\n\u001b[32m    189\u001b[39m image_base_patch_w = base_image_input_size[\u001b[32m1\u001b[39m] // base_image_input_d\n\u001b[32m    190\u001b[39m image_base_patch_h = base_image_input_size[\u001b[32m0\u001b[39m] // base_image_input_d\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m original_image_h, original_image_w = \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m[:\u001b[32m2\u001b[39m]\n\u001b[32m    193\u001b[39m crop_size = base_image_input_size[\u001b[32m0\u001b[39m]\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Discard this many patches from the (left/top, right/bottom) of crops\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "image = Image.open(\"345.png\").convert(\"RGB\")\n",
    "print(f\"✅ Loaded image: {image.size}, mode={image.mode}\")\n",
    "\n",
    "inputs = processor.process(images=[image], text=\"What time is shown?\")\n",
    "inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n",
    "\n",
    "output = model.generate_from_batch(\n",
    "    inputs,\n",
    "    GenerationConfig(max_new_tokens=50),\n",
    "    tokenizer=processor.tokenizer\n",
    ")\n",
    "\n",
    "generated = processor.tokenizer.decode(\n",
    "    output[0, inputs['input_ids'].shape[1]:], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"✅ Output:\", generated.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
