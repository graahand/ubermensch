
| question                                                                                                                                               | answer                                                                                                                                                                                                                                                                      |
| ------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 768-d vector,                                                                                                                                          | its just a 768-dim row or a matrix that represents the patch of the image                                                                                                                                                                                                   |
| what does it means that the [cls] token is learnable?                                                                                                  | [cls] token is like a fake summary patch which is added during training along with the 768-dim vector which is later got learnt and act as a summary to that vector.                                                                                                        |
| what is Q=XnormWQ, K = XnormWk and so on? and why they are performed with those operations and what were they like before the operation was performed. | so the Xnorm is a 768-dim vector representing a patch or a text token after getting normalized. now the attention mechanism converts that one input into three different outputs/vector which allow the model to convert the raw embeddings into contextualized embeddings. |
| what is Qh, Qk, Qv? and why there is sqrt of 64 in the softmax function?                                                                               | the square root is used in order to prevent the problem of gradients explosion during softmax.                                                                                                                                                                              |
| <br>What is Wo means and why is it multipleied with multi-head attention score?                                                                        | Wo is a 768x768 dimensional vector which allow combining the information from all the heads intelligently instead of just mere concatenation.                                                                                                                               |
1

